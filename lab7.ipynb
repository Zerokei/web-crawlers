{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫实验"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1-1.7 学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "response = urllib.request.urlopen('https://www.baidu.com')\n",
    "print(response.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "data = bytes(urllib.parse.urlencode({'Chritch': '3200104207'}), encoding='utf8')\n",
    "response = urllib.request.urlopen('https://httpbin.org/post', data=data)\n",
    "print(response.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "\n",
    "text = '''\n",
    "<html><body><div>\n",
    "<ul>\n",
    "<li class=\"0\"> first  </li>\n",
    "<li class=\"1\"> second </li>\n",
    "<li class=\"1\"> third  </li>\n",
    "<li class=\"0\"> fourth </li>\n",
    "</ul>\n",
    "</div></body></html>\n",
    "'''\n",
    "\n",
    "html = etree.HTML(text)\n",
    "result = html.xpath('//li')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "\n",
    "text = '''\n",
    "<html><body><div>\n",
    "<ul>\n",
    "<li class=\"0\"> first  </li>\n",
    "<li class=\"1\"> second </li>\n",
    "<li class=\"1\"> third  </li>\n",
    "<li class=\"0\"> fourth </li>\n",
    "</ul>\n",
    "</div></body></html>\n",
    "'''\n",
    "\n",
    "html = etree.HTML(text)\n",
    "result = html.xpath('//li[@class=\"0\"]')\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree \n",
    "\n",
    "text = '''\n",
    "<html><body><div>\n",
    "<ul>\n",
    "<li class=\"0\"> first  </li>\n",
    "<li class=\"1\"> second </li>\n",
    "<li class=\"1\"> third  </li>\n",
    "<li class=\"0\"> fourth </li>\n",
    "</ul>\n",
    "</div></body></html>\n",
    "'''\n",
    "\n",
    "html = etree.HTML(text)\n",
    "result = html.xpath('//li[@class=\"0\"]/text()')\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 爬取猫眼电影 top100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8.1 爬取 html\n",
    "因为猫眼电影近些年的反爬机制变严格了，故采取了一些比较特殊的手段\n",
    "\n",
    "- 根据文章指示，在谷歌浏览器调试模式下，copy as cURL(bash)，然后使用 curl-converter 转换成 python 代码\n",
    "- 注意：如果爬取的内容为输入验证信息，则需要现在浏览器中验证后，再重复上述步骤（保证没有被猫眼ban掉）\n",
    "- references\n",
    "    - https://blog.csdn.net/weixin_40340586/article/details/120134102\n",
    "    - https://curlconverter.com/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "\n",
    "cookies = {\n",
    "    '__mta': '217962025.1672026973241.1672052779855.1672052791315.27',\n",
    "    'uuid_n_v': 'v1',\n",
    "    'uuid': '3D291FD084D111EDBB2321DFA394E4C280D7CD55FEE54EFD943630063FB059E1',\n",
    "    '_csrf': '65bfbba146b96ab86c672c8aa5062a9350ff0a51eedbb3ce81a2af58b2c3ad96',\n",
    "    '_lxsdk_cuid': '181aa0bc24713-09a91303c0c072-26021a51-240000-181aa0bc248c8',\n",
    "    '_lxsdk': '3D291FD084D111EDBB2321DFA394E4C280D7CD55FEE54EFD943630063FB059E1',\n",
    "    'Hm_lvt_703e94591e87be68cc8da0da7cbd0be2': '1672026972',\n",
    "    '__mta': '142375437.1672026977996.1672026977996.1672026977996.1',\n",
    "    'Hm_lpvt_703e94591e87be68cc8da0da7cbd0be2': '1672052791',\n",
    "    '_lxsdk_s': '1854e1b83af-6b6-637-298%7C%7C11',\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'Accept-Language': 'zh,zh-TW;q=0.9,en-US;q=0.8,en;q=0.7,zh-CN;q=0.6',\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    'Connection': 'keep-alive',\n",
    "    # 'Cookie': '__mta=217962025.1672026973241.1672052779855.1672052791315.27; uuid_n_v=v1; uuid=3D291FD084D111EDBB2321DFA394E4C280D7CD55FEE54EFD943630063FB059E1; _csrf=65bfbba146b96ab86c672c8aa5062a9350ff0a51eedbb3ce81a2af58b2c3ad96; _lxsdk_cuid=181aa0bc24713-09a91303c0c072-26021a51-240000-181aa0bc248c8; _lxsdk=3D291FD084D111EDBB2321DFA394E4C280D7CD55FEE54EFD943630063FB059E1; Hm_lvt_703e94591e87be68cc8da0da7cbd0be2=1672026972; __mta=142375437.1672026977996.1672026977996.1672026977996.1; Hm_lpvt_703e94591e87be68cc8da0da7cbd0be2=1672052791; _lxsdk_s=1854e1b83af-6b6-637-298%7C%7C11',\n",
    "    'Sec-Fetch-Dest': 'document',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Sec-Fetch-Site': 'none',\n",
    "    'Sec-Fetch-User': '?1',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
    "    'sec-ch-ua': '\"Not?A_Brand\";v=\"8\", \"Chromium\";v=\"108\", \"Google Chrome\";v=\"108\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Windows\"',\n",
    "}\n",
    "\n",
    "\n",
    "pages = []\n",
    "for i in range(10):\n",
    "    # 使用 cookies 以绕过猫眼的验证，offset为当前页的第一部电影排名\n",
    "    response = requests.get('https://www.maoyan.com/board/4?offset={}'.format(i*10), cookies=cookies, headers=headers)\n",
    "    pages.append(response.text)\n",
    "    print(\"已获取第{}页\".format(i+1))\n",
    "    # 间隔 3 秒时间，防止过于频繁\n",
    "    time.sleep(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8.2 提取 top100 电影信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "\n",
    "top_movies = [] \n",
    "\n",
    "def extract_element(text):\n",
    "    print(text)\n",
    "    html = etree.HTML(text)\n",
    "    # 提取各种元素\n",
    "    numbers = html.xpath('//div[@class=\"content\"]/div/div/dl/dd/i/text()')\n",
    "    movies = html.xpath('//div[@class=\"content\"]/div/div/dl/dd/div/div/div/p/a/text()')\n",
    "    stars = html.xpath('//div[@class=\"content\"]/div/div/dl/dd/div/div/div/p[@class=\"star\"]/text()')\n",
    "    releasetimes = html.xpath('//div[@class=\"content\"]/div/div/dl/dd/div/div/div/p[@class=\"releasetime\"]/text()')\n",
    "    scores_interger = html.xpath('//div[@class=\"content\"]/div/div/dl/dd/div/div/div/p[@class=\"score\"]/i[@class=\"integer\"]/text()')\n",
    "    scores_fraction = html.xpath('//div[@class=\"content\"]/div/div/dl/dd/div/div/div/p[@class=\"score\"]/i[@class=\"fraction\"]/text()')\n",
    "    for i in range(10):\n",
    "        movie = []\n",
    "        movie.append(numbers[i])\n",
    "        movie.append(movies[i])\n",
    "        movie.append(stars[i].replace('\\n', '').strip())\n",
    "        movie.append(releasetimes[i])\n",
    "        movie.append(scores_interger[i] + scores_fraction[i])\n",
    "        # 加入top movies列表中\n",
    "        top_movies.append(movie)\n",
    "\n",
    "# 解析1-10页的内容\n",
    "for i in range(10):\n",
    "    extract_element(pages[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8.3 输出到文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open('top_100.txt', 'w').close()\n",
    "for i in range(100):\n",
    "    with open('top_100.txt', 'a') as f:\n",
    "        print(top_movies[i], file=f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 OCR 识别"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装 tesserocr 的库时，可能需要一些库支持\n",
    "```bash\n",
    "$ sudo apt install libleptonica-dev libtesseract-dev tesseract-ocr{,-eng,-osd}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tesserocr\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image = Image.open('checkcode.png')\n",
    "\n",
    "# 处理图像\n",
    "image = image.convert('L')\n",
    "threshold = 150 # 设置滤波上限\n",
    "table = []\n",
    "for i in range(256):\n",
    "    if i < threshold:\n",
    "        table.append(0)\n",
    "    else:\n",
    "        table.append(1)\n",
    "image = image.point(table, '1')\n",
    "\n",
    "# plt.imshow(image)\n",
    "result = tesserocr.image_to_text(image)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 爬取起点中文网免费小说"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.html\n",
    "from lxml import etree\n",
    "import requests\n",
    "import urllib\n",
    "import random\n",
    "import time\n",
    "import random\n",
    "def get_ua():\n",
    "    # https://blog.csdn.net/SL_World/article/details/99647119\n",
    "    first_num = random.randint(55, 76)\n",
    "    third_num = random.randint(0, 3800)\n",
    "    fourth_num = random.randint(0, 140)\n",
    "    os_type = [\n",
    "        '(Windows NT 6.1; WOW64)', '(Windows NT 10.0; WOW64)', '(X11; Linux x86_64)',\n",
    "        '(Machintosh; Inter Mac OS X 10_14_5)'\n",
    "    ]\n",
    "    chrome_version = 'Chrome/{}.0.{}.{}'.format(first_num, third_num, fourth_num)\n",
    "    ua = ' '.join(['Mozilla/5.0', random.choice(os_type), 'AppleWebKit/537.36',\n",
    "    '(KHTML, like Gecko)', chrome_version, 'Safari/537.36'])\n",
    "    return ua\n",
    "\n",
    "def get_html(url):\n",
    "\n",
    "    headers = {\n",
    "        'Referer': 'https://book.qidian.com/',\n",
    "        'User-Agent': get_ua(),\n",
    "    }\n",
    "\n",
    "    # response = requests.get('https://book.qidian.com/info/1034919102/', cookies=cookies, headers=headers)\n",
    "\n",
    "    try: \n",
    "        res = requests.get(url,headers=headers, timeout=(3,4))\n",
    "        try:\n",
    "            res.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as err:\n",
    "            print(f'Error: {err}')\n",
    "        else: \n",
    "            print('Success!')\n",
    "        return res.text\n",
    "    except:\n",
    "        print(res.text)\n",
    "        print(\"爬取超时\")\n",
    "# 输入小说的第一章\n",
    "url = 'https://read.qidian.com/chapter/0mNMNoFQxR3bhZU9AFSCzA2/RqJBDW3NgsP4p8iEw--PPw2/'\n",
    "\n",
    "# pages = []\n",
    "for i in range(30):\n",
    "    print(\"爬取第{}章\".format(i+1))\n",
    "    res = get_html(url)\n",
    "    # print(res)\n",
    "    html = etree.HTML(res)\n",
    "    pages.append(html)\n",
    "    url = \"https:{}\".format(html.xpath('//*[@id=\"j_chapterNext\"]/@href')[0])\n",
    "    print(\"获取链接：{}\".format(url))\n",
    "    time.sleep(random.random()*3)\n",
    "    # break\n",
    "# catalog_page = lxml.html.document_fromstring(get_html(url))\n",
    "# print(catalog_page.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "# print(catalog_page)\n",
    "open('novel.txt', 'w').close()\n",
    "for html in pages:\n",
    "    text = html.xpath('//div[@class=\"read-content j_readContent\"]/p/text()')\n",
    "    for line in text:\n",
    "        with open('novel.txt', 'a') as f:\n",
    "            print(line, file=f)\n",
    "    # print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "遇到的问题：\n",
    "- 动态加载的主页 -> 采用访问第一页，然后顺序向后索引的方式\n",
    "- 获取的html和原有的html内容不一致 -> 参考获取的 html 页内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  requests\n",
    "from fake_useragent import UserAgent\n",
    "from lxml import etree\n",
    "class photo_spider(object):\n",
    "    def __init__(self):\n",
    "        self.url = 'https://book.qidian.com/info/1020580616#Catalog'\n",
    "        ua = UserAgent(verify_ssl=False)\n",
    "        #随机产生user-agent\n",
    "        for i in range(1, 100):\n",
    "            self.headers = {\n",
    "                'User-Agent': ua.random\n",
    "            }\n",
    "    def mian(self):\n",
    "     pass\n",
    "def get_html(self,url):\n",
    "    response=requests.get(url,headers=self.headers)\n",
    "    html=response.content.decode('utf-8')\n",
    "    return html\n",
    "class qidian(object):\n",
    "    def __init__(self):\n",
    "        self.url = 'https://book.qidian.com/info/1020580616#Catalog'\n",
    "        ua = UserAgent(verify_ssl=False)\n",
    "        for i in range(1, 100):\n",
    "            self.headers = {\n",
    "                'User-Agent': ua.random\n",
    "            }\n",
    "    def get_html(self,url):\n",
    "        response=requests.get(url,headers=self.headers)\n",
    "        html=response.content.decode('utf-8')\n",
    "        return html\n",
    "    def parse_html(self,html):\n",
    "        target=etree.HTML(html)\n",
    "        links=target.xpath('//ul[@class=\"cf\"]/li/a/@href')#获取链接\n",
    "        names=target.xpath('//ul[@class=\"cf\"]/li/a/text()')#获取每一章的名字\n",
    "        for link,name in zip(links,names):\n",
    "            print(name+'\\t'+'https:'+link)\n",
    "    def main(self):\n",
    "        url=self.url\n",
    "        html=self.get_html(url)\n",
    "        self.parse_html(html)\n",
    "        print(html)\n",
    "spider=qidian()\n",
    "spider.main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
